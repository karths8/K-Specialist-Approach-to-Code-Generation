{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf3db71-5e9b-4ea5-8e1d-ed27b93125d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/workspace/CS762_Project/Data_files/final_seed_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa4cf50-328a-4162-b462-b7ff56904166",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'local_rank': -1, 'save_total_limit': 5, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'learning_rate': 0.0002, 'max_grad_norm': 0.3, 'weight_decay': 0.001, 'lora_alpha': 16, 'lora_dropout': 0.1, 'lora_r': 64, 'max_seq_length': 512, 'model_name': '/workspace/CS762_Project/CodeLlama-7b-Python-hf', 'dataset_name': '/workspace/CS762_Project/generated_data', 'use_4bit': True, 'use_nested_quant': False, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_type': 'nf4', 'num_train_epochs': 1, 'fp16': True, 'bf16': False, 'packing': False, 'gradient_checkpointing': True, 'optim': 'paged_adamw_32bit', 'lr_scheduler_type': 'constant', 'max_steps': 10000, 'warmup_ratio': 0.03, 'group_by_length': True, 'save_steps': 10, 'logging_steps': 10, 'merge_and_push': False, 'output_dir': './results'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa5fce8d-f383-448a-94c3-efa56794c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "with open('/workspace/CS762_Project/Data_files/final_seed_data.json', 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "for idx in range(len(data)):\n",
    "    data[idx]['cluster'] = random.randint(0,4)\n",
    "clustered_data = {}\n",
    "for i in data:\n",
    "    if i['cluster'] not in clustered_data:\n",
    "        clustered_data[i['cluster']] = []\n",
    "    clustered_data[i['cluster']].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c70da24-1715-4271-ae77-cd2ed60cb8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 16, 4: 17, 1: 8, 2: 9, 3: 8}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:len(v) for k,v in clustered_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5844fe4-2b04-48db-9440-7d67be7a6085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_prepare_model(args):\n",
    "    compute_dtype = getattr(torch, args.bnb_4bit_compute_dtype)\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=args.use_4bit,\n",
    "        bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=args.use_nested_quant,\n",
    "    )\n",
    "\n",
    "    if compute_dtype == torch.float16 and args.use_4bit:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        if major >= 8:\n",
    "            print(\"=\" * 80)\n",
    "            print(\"Your GPU supports bfloat16, you can accelerate training with the argument --bf16\")\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "    # Load the entire model on the GPU 0\n",
    "    # switch to `device_map = \"auto\"` for multi-GPU\n",
    "    device_map = {\"\": 0}\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name, \n",
    "        quantization_config=bnb_config, \n",
    "        device_map=device_map, \n",
    "        use_auth_token=True\n",
    "    )\n",
    "    \n",
    "    # check: https://github.com/huggingface/transformers/pull/24906\n",
    "    model.config.pretraining_tp = 1 \n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        lora_alpha=script_args.lora_alpha,\n",
    "        lora_dropout=script_args.lora_dropout,\n",
    "        r=script_args.lora_r,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\", \n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    return model, peft_config, tokenizer\n",
    "\n",
    "def train_model(script_args):\n",
    "    training_arguments = TrainingArguments(\n",
    "        output_dir=script_args.output_dir,\n",
    "        per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "        gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n",
    "        evaluation_strategy='steps',\n",
    "        eval_steps=script_args.save_steps,\n",
    "        optim=script_args.optim,\n",
    "        save_steps=script_args.save_steps,\n",
    "        logging_steps=script_args.logging_steps,\n",
    "        learning_rate=script_args.learning_rate,\n",
    "        fp16=script_args.fp16,\n",
    "        bf16=script_args.bf16,\n",
    "        max_grad_norm=script_args.max_grad_norm,\n",
    "        # max_steps=script_args.max_steps,\n",
    "        warmup_ratio=script_args.warmup_ratio,\n",
    "        group_by_length=script_args.group_by_length,\n",
    "        lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "        num_train_epochs = script_args.num_train_epochs,\n",
    "        save_total_limit=script_args.save_total_limit,\n",
    "        metric_for_best_model='eval_loss',\n",
    "        # save_steps=script_args.save_steps,\n",
    "        save_strategy='steps'\n",
    "    )\n",
    "    \n",
    "    model, peft_config, tokenizer = create_and_prepare_model(script_args)\n",
    "    model.config.use_cache = False\n",
    "    # dataset = load_dataset(script_args.dataset_name, split=\"train\")\n",
    "    full_dataset = DatasetDict.load_from_disk(script_args.dataset_name)\n",
    "    # Fix weird overflow issue with fp16 training\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset= full_dataset['train'],\n",
    "        eval_dataset = full_dataset['test'],\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"prompt\",\n",
    "        max_seq_length=script_args.max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=script_args.packing\n",
    "    )\n",
    "    \n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d8d50-ac89-4808-8406-c501b53ed2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['/workspace/CS762_Project/phi-2']\n",
    "k_list = [1, 5, 10]\n",
    "for model in model_list:\n",
    "    for k in k_list:\n",
    "        # load data_1, data_5, data_10\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
