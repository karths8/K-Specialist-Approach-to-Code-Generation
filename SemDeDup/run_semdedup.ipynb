{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158be6f2-3408-45c4-b5cc-ad2708664b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pprint\n",
    "from constants import DIST_METRIC_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd6fae8-909a-4a64-af3e-34f9baa9d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_memmap_embs(\n",
    "    embs_memory_loc: str, dataset_size: int, emd_size: int = 512, dtype: str = \"float32\"\n",
    ") -> np.memmap:\n",
    "    \"\"\"\n",
    "    Initializes a memory-mapped NumPy array to read embeddings of examples.\n",
    "\n",
    "    Args:\n",
    "        embs_memory_loc (str): Path to the memory-mapped file.\n",
    "        dataset_size (int): Size of the dataset.\n",
    "        emd_size (int): Dimensionality of the embeddings.\n",
    "        dtype (str): Data type of the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.memmap: A memory-mapped NumPy array.\n",
    "    \"\"\"\n",
    "    embs = np.memmap(\n",
    "        embs_memory_loc, dtype=dtype, mode=\"r\", shape=(dataset_size, emd_size)\n",
    "    )\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c6f7651-1571-44e3-a136-aa595a03dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemDeDup():\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        random.seed(args.seed)\n",
    "\n",
    "    def contains_duplicates(self, arr):\n",
    "        return len(np.unique(arr)) != len(arr)\n",
    "\n",
    "    def semdedup(self, cluster, cluster_reps, device):\n",
    "        st = time.time()\n",
    "        ## -- compute pairwise cos sim between cluster items, then replace to diagonal with zeros to ignore self similarity\n",
    "        cluster_reps.to(device)\n",
    "        pair_w_sim_matrix = cluster_reps @ (cluster_reps.T)\n",
    "        del cluster_reps\n",
    "        pair_w_sim_matrix.fill_diagonal_(0.0)\n",
    "        assert pair_w_sim_matrix.shape[0] == pair_w_sim_matrix.shape[1]\n",
    "\n",
    "        # TODO: what are you doung with image_urls?\n",
    "        ## -- get paths to cluster i images\n",
    "        image_urls = cluster[:, 0]\n",
    "\n",
    "        ## -- make sure all the paths are unique this ensure that the duplicates are really stored many time times on memory\n",
    "        assert not self._contains_duplicates(image_urls)\n",
    "\n",
    "        ## -- We need upper tringular matrix because (1)we don't need to look at self sim (always=1) (2)we need the compinations not permutations\n",
    "        triu_sim_mat = torch.triu(pair_w_sim_matrix, diagonal=1)\n",
    "\n",
    "        ## -- if the max sim between one example and any other example is > 1-eps, remove this example\n",
    "        M = torch.max(triu_sim_mat, dim=0)[0].cpu()\n",
    "        print(f\"Step time: {time.time()-st}(s)\")\n",
    "\n",
    "        return M\n",
    "\n",
    "    def process_clusters(self, start_cluster: int, end_cluster: int):\n",
    "        # print(\"SemDeDup params: \", self.args)\n",
    "        st = time.time()\n",
    "\n",
    "        embs = init_memmap_embs(\n",
    "            self.args.embs_memory_loc, self.args.dataset_size, self.args.emd_size\n",
    "        )\n",
    "\n",
    "        step_time = []\n",
    "\n",
    "        for cluster_id in tqdm(range(start_cluster, end_cluster)):\n",
    "            step_st = time.time()\n",
    "\n",
    "            df_file_loc = os.path.join(\n",
    "                self.args.save_loc, f\"dataframes/cluster_{cluster_id}.pkl\"\n",
    "            )\n",
    "\n",
    "            if os.path.exists(df_file_loc):  # and os.path.exists(dict_file_loc):\n",
    "                print(f\"{df_file_loc} exists, moving on\")\n",
    "                continue\n",
    "\n",
    "            ## -- load cluster i representations\n",
    "            cluster_i = np.load(\n",
    "                os.path.join(\n",
    "                    self.args.sorted_clusters_path, f\"cluster_{cluster_id}.npy\"\n",
    "                )\n",
    "            )\n",
    "            # 1) store cluster size\n",
    "            cluster_size = cluster_i.shape[0]\n",
    "            print(\"cluster_size: \", cluster_size)\n",
    "\n",
    "            if cluster_size == 1:\n",
    "                points_to_remove_df = pd.DataFrame()\n",
    "                points_to_remove_df[\"indices\"] = [0]\n",
    "                for eps in self.args.eps_list:\n",
    "                    ## We need to remove a point from the dataset when its pairwise similarity to other point is > 1-ebs\n",
    "                    points_to_remove_df[f\"eps={eps}\"] = [False]\n",
    "                if self.args.save_loc != \"\":\n",
    "                    ## --save df\n",
    "                    with open(df_file_loc, \"wb\") as file:\n",
    "                        pickle.dump(points_to_remove_df, file)\n",
    "                print(\"DONE cluster_id \", cluster_id)\n",
    "                continue\n",
    "\n",
    "            ## -- By default, we keep hard examples from groups\n",
    "            clutser_items_indices = list(range(cluster_size))\n",
    "            ## -- OR: shuffle cluster to keep random example from each group\n",
    "            if self.args.which_to_keep.lower() == \"random\":\n",
    "                random.shuffle(clutser_items_indices)\n",
    "                cluster_i = cluster_i[clutser_items_indices]\n",
    "            ## -- OR: reverse cluster to keep easy examples\n",
    "            if self.args.which_to_keep.lower() == \"easy\":\n",
    "                clutser_items_indices = clutser_items_indices[::-1]\n",
    "                cluster_i = cluster_i[clutser_items_indices]\n",
    "\n",
    "            ## -- indices for cluster items in the dataset\n",
    "            cluster_ids = cluster_i[:, 1].astype(\"int32\")\n",
    "            cluster_reps = embs[cluster_ids]\n",
    "            cluster_reps = torch.tensor(cluster_reps)\n",
    "\n",
    "            M = self.semdedup(cluster_i, cluster_reps, self.args.device)\n",
    "\n",
    "            points_to_remove_df = pd.DataFrame()\n",
    "            points_to_remove_df[\"indices\"] = clutser_items_indices\n",
    "\n",
    "            for eps in self.args.eps_list:\n",
    "                ## -- 5) We need to remove a point from the dataset when its pairwise similarity to other point is > 1-ebs\n",
    "                eps_points_to_remove = M > 1 - eps\n",
    "                points_to_remove_df[f\"eps={eps}\"] = eps_points_to_remove\n",
    "\n",
    "            if self.args.save_loc != \"\":\n",
    "                ## --save df\n",
    "                with open(df_file_loc, \"wb\") as file:\n",
    "                    pickle.dump(points_to_remove_df, file)\n",
    "\n",
    "            step_time.append_cluster(time.time() - step_st)\n",
    "            print(\"DONE cluster: \", cluster_id)\n",
    "\n",
    "        print(\n",
    "            f\"DONE in {((time.time()-st)/60):.2f} minutes, Average Step time {(sum(step_time)/len(step_time)):.2f}(s)\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def __call__(self):\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(vars(self.args))\n",
    "        self._process_shard(start_cluster, end_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abdaefe-3717-4743-8529-1f77b17e49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"seed\": 5,\n",
    "    \"embs_memory_loc\": \"\",\n",
    "    \"dataset_size\": ,\n",
    "    \"emd_size\": ,\n",
    "    \"save_loc\": ,\n",
    "    \"sorted_clusters_path\": ,\n",
    "    \"eps_list\": ,\n",
    "    \"which_to_keep\": ,\n",
    "    \"device\": ,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
