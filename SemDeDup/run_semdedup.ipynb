{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dc1fc7-7443-4503-af83-3966afee0f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158be6f2-3408-45c4-b5cc-ad2708664b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pprint\n",
    "from constants import DIST_METRIC_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd6fae8-909a-4a64-af3e-34f9baa9d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_memmap_embs(\n",
    "    embs_memory_loc: str, dataset_size: int, emd_size: int = 512, dtype: str = \"float32\"\n",
    ") -> np.memmap:\n",
    "    \"\"\"\n",
    "    Initializes a memory-mapped NumPy array to read embeddings of examples.\n",
    "\n",
    "    Args:\n",
    "        embs_memory_loc (str): Path to the memory-mapped file.\n",
    "        dataset_size (int): Size of the dataset.\n",
    "        emd_size (int): Dimensionality of the embeddings.\n",
    "        dtype (str): Data type of the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        np.memmap: A memory-mapped NumPy array.\n",
    "    \"\"\"\n",
    "    embs = np.memmap(\n",
    "        embs_memory_loc, dtype=dtype, mode=\"r\", shape=(dataset_size, emd_size)\n",
    "    )\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c6f7651-1571-44e3-a136-aa595a03dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemDeDup():\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        random.seed(args[\"seed\"])\n",
    "\n",
    "    def contains_duplicates(self, arr):\n",
    "        return len(np.unique(arr)) != len(arr)\n",
    "\n",
    "    def semdedup(self, cluster, cluster_reps, device):\n",
    "        st = time.time()\n",
    "        ## -- compute pairwise cos sim between cluster items, then replace to diagonal with zeros to ignore self similarity\n",
    "        cluster_reps.to(device)\n",
    "        pair_w_sim_matrix = cluster_reps @ (cluster_reps.T)\n",
    "        del cluster_reps\n",
    "        pair_w_sim_matrix.fill_diagonal_(0.0)\n",
    "        assert pair_w_sim_matrix.shape[0] == pair_w_sim_matrix.shape[1]\n",
    "\n",
    "        # TODO: what are you doung with image_urls?\n",
    "        ## -- get paths to cluster i images\n",
    "        image_urls = cluster[:, 0]\n",
    "\n",
    "        ## -- make sure all the paths are unique this ensure that the duplicates are really stored many time times on memory\n",
    "        assert not self.contains_duplicates(image_urls)\n",
    "\n",
    "        ## -- We need upper tringular matrix because (1)we don't need to look at self sim (always=1) (2)we need the compinations not permutations\n",
    "        triu_sim_mat = torch.triu(pair_w_sim_matrix, diagonal=1)\n",
    "\n",
    "        ## -- if the max sim between one example and any other example is > 1-eps, remove this example\n",
    "        M = torch.max(triu_sim_mat, dim=0)[0].cpu()\n",
    "        print(f\"Step time: {time.time()-st}(s)\")\n",
    "\n",
    "        return M\n",
    "\n",
    "    def process_clusters(self, start_cluster: int, end_cluster: int):\n",
    "        # print(\"SemDeDup params: \", self.args)\n",
    "        st = time.time()\n",
    "\n",
    "        embs = init_memmap_embs(\n",
    "            self.args[\"embs_memory_loc\"], self.args[\"dataset_size\"], self.args[\"emd_size\"]\n",
    "        )\n",
    "\n",
    "        step_time = []\n",
    "\n",
    "        for cluster_id in tqdm(range(start_cluster, end_cluster)):\n",
    "            step_st = time.time()\n",
    "\n",
    "            df_file_loc = os.path.join(\n",
    "                self.args[\"save_loc\"], f\"dataframes/cluster_{cluster_id}.pkl\"\n",
    "            )\n",
    "\n",
    "            if os.path.exists(df_file_loc):  # and os.path.exists(dict_file_loc):\n",
    "                print(f\"{df_file_loc} exists, moving on\")\n",
    "                continue\n",
    "\n",
    "            ## -- load cluster i representations\n",
    "            cluster_i = np.load(\n",
    "                os.path.join(\n",
    "                    self.args[\"sorted_clusters_path\"], f\"cluster_{cluster_id}.npy\"\n",
    "                )\n",
    "            )\n",
    "            # 1) store cluster size\n",
    "            cluster_size = cluster_i.shape[0]\n",
    "            print(\"cluster_size: \", cluster_size)\n",
    "\n",
    "            if cluster_size == 1:\n",
    "                points_to_remove_df = pd.DataFrame()\n",
    "                points_to_remove_df[\"indices\"] = [0]\n",
    "                for eps in self.args[\"eps_list\"]:\n",
    "                    ## We need to remove a point from the dataset when its pairwise similarity to other point is > 1-ebs\n",
    "                    points_to_remove_df[f\"eps={eps}\"] = [False]\n",
    "                if self.args[\"save_loc\"] != \"\":\n",
    "                    ## --save df\n",
    "                    with open(df_file_loc, \"wb\") as file:\n",
    "                        pickle.dump(points_to_remove_df, file)\n",
    "                print(\"DONE cluster_id \", cluster_id)\n",
    "                continue\n",
    "\n",
    "            ## -- By default, we keep hard examples from groups\n",
    "            clutser_items_indices = list(range(cluster_size))\n",
    "            ## -- OR: shuffle cluster to keep random example from each group\n",
    "            if self.args[\"which_to_keep\"].lower() == \"random\":\n",
    "                random.shuffle(clutser_items_indices)\n",
    "                cluster_i = cluster_i[clutser_items_indices]\n",
    "            ## -- OR: reverse cluster to keep easy examples\n",
    "            if self.args[\"which_to_keep\"].lower() == \"easy\":\n",
    "                clutser_items_indices = clutser_items_indices[::-1]\n",
    "                cluster_i = cluster_i[clutser_items_indices]\n",
    "\n",
    "            ## -- indices for cluster items in the dataset\n",
    "            cluster_ids = cluster_i[:, 1].astype(\"int32\")\n",
    "            cluster_reps = embs[cluster_ids]\n",
    "            cluster_reps = torch.tensor(cluster_reps)\n",
    "\n",
    "            M = self.semdedup(cluster_i, cluster_reps, self.args[\"device\"])\n",
    "\n",
    "            points_to_remove_df = pd.DataFrame()\n",
    "            points_to_remove_df[\"indices\"] = clutser_items_indices\n",
    "\n",
    "            for eps in self.args[\"eps_list\"]:\n",
    "                ## -- 5) We need to remove a point from the dataset when its pairwise similarity to other point is > 1-ebs\n",
    "                eps_points_to_remove = M > 1 - eps\n",
    "                points_to_remove_df[f\"eps={eps}\"] = eps_points_to_remove\n",
    "\n",
    "            if self.args[\"save_loc\"] != \"\":\n",
    "                ## --save df\n",
    "                with open(df_file_loc, \"wb\") as file:\n",
    "                    pickle.dump(points_to_remove_df, file)\n",
    "\n",
    "            step_time.append(time.time() - step_st)\n",
    "            print(\"DONE cluster: \", cluster_id)\n",
    "\n",
    "        print(\n",
    "            f\"DONE in {((time.time()-st)/60):.2f} minutes, Average Step time {(sum(step_time)/len(step_time)):.2f}(s)\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    def call(self):\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        pp.pprint(self.args)\n",
    "        start_cluster = 0\n",
    "        end_cluster = 19\n",
    "        self.process_clusters(start_cluster, end_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5abdaefe-3717-4743-8529-1f77b17e49cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"seed\": 5,\n",
    "    \"embs_memory_loc\": \"/workspace/CS762_Project/SemDeDup/code_alpaca_results/emb_mmap.dat\",\n",
    "    \"dataset_size\": 200,\n",
    "    \"emd_size\": 1024,\n",
    "    \"save_loc\": \"/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location\",\n",
    "    \"sorted_clusters_path\": \"/workspace/CS762_Project/SemDeDup/code_alpaca_results/sorted_clusters\",\n",
    "    \"eps_list\": [0.9, 0.95, 0.98],\n",
    "    \"which_to_keep\": \"random\",\n",
    "    \"device\": \"cuda\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3543389-e272-4078-84a2-7c2b11ac22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "semdedup = SemDeDup(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7708d638-9465-4bb5-ad92-9465f6c588f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'dataset_size': 200,\n",
      "    'device': 'cuda',\n",
      "    'embs_memory_loc': '/workspace/CS762_Project/SemDeDup/code_alpaca_results/emb_mmap.dat',\n",
      "    'emd_size': 1024,\n",
      "    'eps_list': [0.9, 0.95, 0.98],\n",
      "    'save_loc': '/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location',\n",
      "    'seed': 5,\n",
      "    'sorted_clusters_path': '/workspace/CS762_Project/SemDeDup/code_alpaca_results/sorted_clusters',\n",
      "    'which_to_keep': 'random'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 15010.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_0.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_1.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_2.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_3.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_4.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_5.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_6.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_7.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_8.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_9.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_10.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_11.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_12.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_13.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_14.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_15.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_16.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_17.pkl exists, moving on\n",
      "/workspace/CS762_Project/SemDeDup/code_alpaca_results/save_location/dataframes/cluster_18.pkl exists, moving on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msemdedup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 123\u001b[0m, in \u001b[0;36mSemDeDup.call\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m start_cluster \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    122\u001b[0m end_cluster \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m19\u001b[39m\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_clusters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_cluster\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_cluster\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 114\u001b[0m, in \u001b[0;36mSemDeDup.process_clusters\u001b[0;34m(self, start_cluster, end_cluster)\u001b[0m\n\u001b[1;32m    110\u001b[0m     step_time\u001b[38;5;241m.\u001b[39mappend(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m step_st)\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDONE cluster: \u001b[39m\u001b[38;5;124m\"\u001b[39m, cluster_id)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDONE in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m((time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mst)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes, Average Step time \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstep_time\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstep_time\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(s)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "semdedup.call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216db57-671a-42c2-b618-a4e979bc6170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
